{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshinohar/langchain-book/blob/main/langchain_ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUj2y-stek1W"
      },
      "source": [
        "# 4. LangChain解説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4oSu5DqWgpG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your api key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3cSlNQV9ZBB"
      },
      "source": [
        "## 4-1 LangChainの概要"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqgjYwQUemjV",
        "outputId": "ba1a6a09-0334-4c09-ffe9-46626e118589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.322-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.51-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, openai, dataclasses-json, langchain\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.322 langsmith-0.0.51 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMModXMNetUP"
      },
      "source": [
        "## 4-2 Language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3elDRjh9jVa"
      },
      "source": [
        "### LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3sdyngepk-",
        "outputId": "9c878d64-a217-441c-f4e1-c27134666799",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "はじめまして、私は○○と申します。現在、○○大学で学んでいます。趣味は料理や旅行などです。特に料理は好きで、毎日新しいレシピを試しています。今までに海外旅行も何回かしていて、異文化に触れるのが大好きです。今後も新しいことに挑戦していきたいと思っています。よろしくお願いします。\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "result = llm(\"自己紹介してください。\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvqkiiI-9mI9"
      },
      "source": [
        "### Chat models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLI_K6YIewJn",
        "outputId": "ccc56f1c-8aee-42fb-b704-4e5cbec21f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "はい、先ほどおっしゃった通り、あなたの名前はジョンですね。何か他にお手伝いできることはありますか？\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"こんにちは！私はジョンと言います！\"),\n",
        "    AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "    HumanMessage(content= \"私の名前が分かりますか？\")\n",
        "]\n",
        "\n",
        "result = chat(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLRYe3Ec9qAO"
      },
      "source": [
        "### Callbackを使ったストリーミング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLxGXw7zgXlR",
        "outputId": "632770bf-9adc-4072-f3ec-2f5b1cd70be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "はい、私はオープンAIの言語モデルであるGPT-3です。私は自然言語処理の能力を持ち、様々なトピックについての質問や会話に応えることができます。私は大量のデータを学習しており、文章の生成や意味の理解、文脈に基づいた回答などを行うことができます。私の目標は、人々がより簡単に情報を得たり、問題を解決したりするのを支援することです。どのようにお手伝いできますか？"
          ]
        }
      ],
      "source": [
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        ")\n",
        "\n",
        "messages = [HumanMessage(content=\"自己紹介してください\")]\n",
        "result = chat(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRH_6qBQnEjZ"
      },
      "source": [
        "## 4-3 Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvnkWfA9tIT"
      },
      "source": [
        "### Prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLfdULPOjUDY",
        "outputId": "745f687f-400f-4496-fcf9-ae7782bdd59a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "以下の料理のレシピを考えてください。\n",
            "\n",
            "料理名: カレー\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "以下の料理のレシピを考えてください。\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   input_variables=[\"dish\"],\n",
        "   template=template,\n",
        ")\n",
        "\n",
        "result = prompt.format(dish=\"カレー\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYyxXfj9viN"
      },
      "source": [
        "### ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUvu-76mnWBK",
        "outputId": "ecfb30e2-83ab-4594-a463-6080ba61b74f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='あなたはイギリス料理のプロフェッショナルです。'), HumanMessage(content='以下の料理のレシピを考えてください。\\n\\n料理名: 肉じゃが')]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"あなたは{country}料理のプロフェッショナルです。\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"以下の料理のレシピを考えてください。\\n\\n料理名: {dish}\")\n",
        "])\n",
        "\n",
        "messages = chat_prompt.format_prompt(country=\"イギリス\", dish=\"肉じゃが\").to_messages()\n",
        "\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-WsdEpEsNta",
        "outputId": "4021bafd-8999-4f7b-c7a0-9ad7baf61d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "材料:\n",
            "- 牛肉 (500g)\n",
            "- じゃがいも (4個)\n",
            "- にんじん (1本)\n",
            "- 玉ねぎ (1個)\n",
            "- にんにく (2片)\n",
            "- トマト缶 (1缶)\n",
            "- しょうゆ (大さじ2)\n",
            "- 砂糖 (大さじ1)\n",
            "- ケチャップ (大さじ2)\n",
            "- ベジタブルストック (200ml)\n",
            "- オリーブオイル (大さじ2)\n",
            "- 塩 (お好みで)\n",
            "- 黒こしょう (お好みで)\n",
            "- パセリ (お好みで)\n",
            "\n",
            "作り方:\n",
            "1. じゃがいも、にんじん、玉ねぎを一口サイズに切ります。\n",
            "2. 牛肉を食べやすい大きさに切ります。\n",
            "3. にんにくをみじん切りにします。\n",
            "4. 大きめの鍋にオリーブオイルを熱し、にんにくを炒めます。\n",
            "5. 牛肉を加えて炒め、全体に焼き色がつくまで炒めます。\n",
            "6. じゃがいも、にんじん、玉ねぎを加えてさらに炒めます。\n",
            "7. トマト缶、しょうゆ、砂糖、ケチャップ、ベジタブルストックを加え、全体がひたひたになるまで水を足します。\n",
            "8. 弱火にして、蓋をして約30分間煮込みます。途中で塩や黒こしょうで味を調えます。\n",
            "9. 煮込みが終わったら、パセリを散らして完成です。\n",
            "\n",
            "肉じゃがはご飯やパンと一緒に食べると美味しいです。お好みでトッピングやアレンジを加えても楽しめます。\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "result = chat(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9SAbwnNwFQq"
      },
      "source": [
        "## 4-4 Output Parsers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gUPEasNNBlP"
      },
      "source": [
        "### PydanticOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEkmkZixwEdi"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aosaD-s-wExZ"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blhjs33M-AHZ",
        "outputId": "2e9c4cf8-0585-427b-9668-81335d8466da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"description\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"steps to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21EfCn8b-AFY"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"料理のレシピを考えてください。\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   template=template,\n",
        "   input_variables=[\"dish\"],\n",
        "   partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Y0KvYk-ADU",
        "outputId": "55658281-cba1-4346-847d-933ecd18eafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "料理のレシピを考えてください。\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"description\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"steps to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "\n",
            "料理名: カレー\n",
            "\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = prompt.format(dish=\"カレー\")\n",
        "\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gQjKsGy9__B",
        "outputId": "ef6e2803-277d-4cb5-ca56-beb503fade77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ingredients\": [\n",
            "    \"玉ねぎ\",\n",
            "    \"にんじん\",\n",
            "    \"じゃがいも\",\n",
            "    \"豚肉\",\n",
            "    \"カレールー\",\n",
            "    \"水\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"玉ねぎ、にんじん、じゃがいもを切る\",\n",
            "    \"豚肉を炒める\",\n",
            "    \"野菜を加えて炒める\",\n",
            "    \"水を加えて煮込む\",\n",
            "    \"カレールーを加えて溶かす\",\n",
            "    \"煮込んで完成\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "messages = [HumanMessage(content=formatted_prompt)]\n",
        "output = chat(messages)\n",
        "\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY4cBqNZ9_1x",
        "outputId": "4e7fcce2-6443-418d-b312-8547ba13bbd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '豚肉', 'カレールー', '水'] steps=['玉ねぎ、にんじん、じゃがいもを切る', '豚肉を炒める', '野菜を加えて炒める', '水を加えて煮込む', 'カレールーを加えて溶かす', '煮込んで完成']\n"
          ]
        }
      ],
      "source": [
        "recipe = parser.parse(output.content)\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUVnoApz-v9O"
      },
      "source": [
        "## 4-4 Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU0czqQn-xgL"
      },
      "source": [
        "### LLMChain―PromptTemplate・Language model・OutputParserをつなぐ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpPIlgjpyroZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "template = \"\"\"料理のレシピを考えてください。\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   template=template,\n",
        "   input_variables=[\"dish\"],\n",
        "   partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jys6cH41C4NW",
        "outputId": "4b002acd-c154-47cd-9ce0-51ead461b8df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '豚肉', 'カレールー', '水'] steps=['玉ねぎ、にんじん、じゃがいもを切る', '豚肉を炒める', '野菜を加えて炒める', '水を加えて煮込む', 'カレールーを加えて溶かす', '煮込んで完成']\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=chat, output_parser=output_parser)\n",
        "\n",
        "recipe = chain.run(dish=\"カレー\")\n",
        "\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCt-YzvX-z4k"
      },
      "source": [
        "### SimpleSequentialChain―ChainとChainをつなぐ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcL-GWtO-0v4"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "cot_template = \"\"\"以下の質問に回答してください。\n",
        "\n",
        "質問: {question}\n",
        "\n",
        "ステップバイステップで考えましょう。\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt = PromptTemplate(\n",
        "   input_variables=[\"question\"],\n",
        "   template=cot_template,\n",
        ")\n",
        "\n",
        "cot_chain = LLMChain(llm=chat, prompt=cot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTWjr3rKBCX"
      },
      "outputs": [],
      "source": [
        "summarize_template = \"\"\"以下の文章を結論だけ一言に要約してください。\n",
        "\n",
        "{input}\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "   input_variables=[\"input\"],\n",
        "   template=summarize_template,\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYGaG5tuKEVX",
        "outputId": "7bf2f245-be9d-4a6c-cdeb-cf6aaf305cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残りのリンゴは10個です。\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "cot_summarize_chain = SimpleSequentialChain(chains=[cot_chain, summarize_chain])\n",
        "\n",
        "result = cot_summarize_chain(\n",
        "   \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\"\n",
        ")\n",
        "print(result[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8rh_PEdw2qZ"
      },
      "source": [
        "上記の入力は [Chain-of-Thoughtプロンプティング | Prompt Engineering Guide](https://www.promptingguide.ai/jp/techniques/cot) から引用しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jpeYc6ARqBz"
      },
      "source": [
        "## 4-5 Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yOh1zrMRrbn"
      },
      "source": [
        "### ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WugAW9-LKGJ0",
        "outputId": "86f50146-0af8-4a58-9622-a8bffa778ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello, I'm Masato.\n",
            "AI: Hello Masato! How can I assist you today?\n",
            "You: Do you know my name?\n",
            "AI: Yes, your name is Masato. You mentioned it at the beginning of our conversation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e6ede111da69>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mai_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI: {ai_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "#chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.run(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRCp43RSw2qa"
      },
      "source": [
        "### （コラム）Chat modelsでMemoryを使う場合の注意"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoEzLBS7KLmH",
        "outputId": "af17b8e7-f867-4d80-c99a-8d75e42d587b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "はい、先ほどおっしゃった通り、あなたの名前はジョンですよ。何か他にお手伝いできることはありますか？\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"こんにちは！私はジョンと言います！\"),\n",
        "    AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "    HumanMessage(content= \"私の名前が分かりますか？\")\n",
        "]\n",
        "\n",
        "result = chat(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CiOdkoJt3pjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}