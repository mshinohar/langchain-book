{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshinohar/langchain-book/blob/main/udemy_langchain_ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Udemyè¬›åº§ã€ŒLangChainã«ã‚ˆã‚‹LLMã‚¢ãƒ—ãƒªé–‹ç™ºå…¥é–€ã€ã®ã‚³ãƒ¼ã‚¹å‰åŠã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰"
      ],
      "metadata": {
        "id": "0LSkqxoVN3J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GPT ã® API ã®åŸºç¤çŸ¥è­˜"
      ],
      "metadata": {
        "id": "IVs2pKa2ODmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. OpenAI ã® API ã‚­ãƒ¼ã‚’ç™ºè¡Œ"
      ],
      "metadata": {
        "id": "mLGNNT3BOWFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "r0cuW-PeOD_s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. OpenAI ã® Completions API ã«ãµã‚Œã‚‹"
      ],
      "metadata": {
        "id": "bYVHgMnrOez3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://api.openai.com/v1/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "}\n",
        "data = {\n",
        "    # å‹•ç”»å†…ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã€Œtext-davinci-003ã€ã¯2024å¹´1æœˆ4æ—¥ã«å»ƒæ­¢äºˆå®šã§ã™ã€‚\n",
        "    # ãã®ãŸã‚ã€ã“ã¡ã‚‰ã®ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€\n",
        "    # Completions APIã§æ–°ã—ãåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸã€Œgpt-3.5-turbo-instructã€ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚\n",
        "    \"model\": \"gpt-3.5-turbo-instruct\",\n",
        "    \"prompt\": \"hello!\",\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "response = requests.post(url=url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "2cPPmp7IOGbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60064ab-83e5-4a26-d371-955d51698bb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-8YP1PGObCiwKrXKjRDGzAiKnQTTSQ\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1703210027,\n",
            "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \"\\n\\nHello! How can I assist you?\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 2,\n",
            "    \"completion_tokens\": 9,\n",
            "    \"total_tokens\": 11\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. OpenAI ã® Chat API ã«ãµã‚Œã‚‹"
      ],
      "metadata": {
        "id": "WY7cd-EeOwiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://api.openai.com/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"hello!\"}\n",
        "    ],\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "response = requests.post(url=url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "bOzgI4KGOIcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785d766f-4efe-420a-c244-07e8c37eaeff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YP2EIifaAwWKfM3POpJFO8MhbI0f\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703210078,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Hello! How can I assist you today?\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 9,\n",
            "    \"completion_tokens\": 9,\n",
            "    \"total_tokens\": 18\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. LangChain å…¥é–€ï¼ˆå‰åŠï¼‰"
      ],
      "metadata": {
        "id": "UlDl6XqsOzFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Modelsï¼ˆLLMsãƒ»Chat Modelsï¼‰"
      ],
      "metadata": {
        "id": "SVO4MzQxPJDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain==0.0.172 openai==0.27.6"
      ],
      "metadata": {
        "id": "lVxZFGSagE1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b12b0fd-832e-4b6d-be80-f8cc361d28b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.9/849.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ã“ã®ã‚»ãƒ«ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã€Œtext-davinci-003ã€ã¯2024å¹´1æœˆ4æ—¥ã«å»ƒæ­¢äºˆå®šã§ã™ã€‚\n",
        "# ãã®å¾Œã€Completions APIã§ã¯ã€Œgpt-3.5-turbo-instructã€ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "#\n",
        "# ãŸã ã—ã€ã“ã®ã‚³ãƒ¼ã‚¹ã§ä½¿ç”¨ã—ã¦ã„ã‚‹LangChain v0.0.172ã¯ã€Œgpt-3.5-turbo-instructã€ã«å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€\n",
        "# ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã€Œgpt-3.5-turbo-instructã€ã«å¤‰æ›´ã—ã¦ã‚‚å‹•ä½œã—ã¾ã›ã‚“ã€‚\n",
        "# (æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®LangChainã§ã‚ã‚Œã°ã€ã€Œgpt-3.5-turbo-instructã€ã‚‚ä½¿ç”¨å¯èƒ½ã§ã™)\n",
        "#\n",
        "# LangChainã®OpenAIã‚¯ãƒ©ã‚¹ã‚„Completions APIã¯ã“ã®ã‚³ãƒ¼ã‚¹ã§ä»¥å¾Œä½¿ç”¨ã—ãªã„ãŸã‚ã€\n",
        "# ã“ã®ã‚³ãƒ¼ãƒ‰ãŒå‹•ä½œã—ãªãã¦ã‚‚ã‚ã¾ã‚Šæ°—ã«ã›ãšã€è¬›åº§ã®ç¶šãã«é€²ã‚“ã§ãã ã•ã„ã€‚\n",
        "# (ã‚‚ã—ã‚‚OpenAIã‚¯ãƒ©ã‚¹ã‚’ã€Œgpt-3.5-turbo-instructã€ã§å‹•ã‹ã—ã¦ã¿ãŸã„å ´åˆã¯ã€\n",
        "# Google Colabã‚’åˆ¥é€”ç”¨æ„ã—ã¦ã€LangChainã®æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„)\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "result = llm.predict(\"è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "gJHDUhwwgKc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df7a56-d79b-43e9-9157-ec0956ba5911"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ã¯ã˜ã‚ã¾ã—ã¦ã€ç§ã¯â—‹â—‹ã¨ç”³ã—ã¾ã™ã€‚ç¾åœ¨ã€â—‹â—‹å¤§å­¦ã§å­¦ã‚“ã§ã„ã¾ã™ã€‚è¶£å‘³ã¯æ–™ç†ã‚„æ—…è¡Œãªã©ã§ã™ã€‚ç‰¹ã«æ–™ç†ã¯å¥½ãã§ã€æ¯æ—¥æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã‚’è©¦ã—ã¦ã„ã¾ã™ã€‚ä»Šã¾ã§ã«æµ·å¤–æ—…è¡Œã‚‚ä½•å›ã‹ã—ã¦ã„ã¦ã€ç•°æ–‡åŒ–ã«è§¦ã‚Œã‚‹ã®ãŒå¤§å¥½ãã§ã™ã€‚ä»Šå¾Œã‚‚æ–°ã—ã„ã“ã¨ã«æŒ‘æˆ¦ã—ã¦ã„ããŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "result = chat.predict(\"è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "OShyvz70hRD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06038815-899a-4c51-eb5d-291cf718ef74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯AIã§ã™ã€‚ç§ã¯OpenAIãŒé–‹ç™ºã—ãŸè‡ªç„¶è¨€èªå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ç§ã®ç›®çš„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè³ªå•ã‚„è¦æ±‚ã‚’ã™ã‚‹ã¨ã€æœ€å–„ã®å›ç­”ã‚„å¿œç­”ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ç§ã¯æ§˜ã€…ãªãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã®æƒ…å ±ã‚’æŒã£ã¦ãŠã‚Šã€æ–‡æ³•ã‚„ã‚¹ãƒšãƒ«ã®ä¿®æ­£ã‚‚è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Promptsï¼ˆPrompt Templatesï¼‰"
      ],
      "metadata": {
        "id": "z9XTxsN8xitv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã®æ¦‚è¦ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ã‚³ãƒãƒ³ãƒ‰: {command}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"command\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "result = prompt.format(command=\"ls\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "-sZIx1e8iwJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ef6fb1-ec59-46da-f830-9a0a5d1a18cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã®æ¦‚è¦ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
            "\n",
            "ã‚³ãƒãƒ³ãƒ‰: ls\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Chains"
      ],
      "metadata": {
        "id": "u0eKtJGu0vX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "# Model ã‚’ç”¨æ„\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Prompt ã‚’ç”¨æ„\n",
        "template = \"\"\"\n",
        "æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã®æ¦‚è¦ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ã‚³ãƒãƒ³ãƒ‰: {command}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"command\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Chain ã‚’ä½œæˆ\n",
        "chain = LLMChain(llm=chat, prompt=prompt)\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "result = chain.run(\"ls\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "9QyYR_Iixyuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059189c6-10b6-4d35-edc6-1878ebc85653"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã®æ¦‚è¦ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
            "\n",
            "ã‚³ãƒãƒ³ãƒ‰: ls\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "lsã‚³ãƒãƒ³ãƒ‰ã¯ã€Unixã‚„Unixç³»ã®ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚³ãƒãƒ³ãƒ‰ã§ã€æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸€è¦§ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ãŒã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ä»–ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åå‰ã€ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€æ‰€æœ‰è€…ã€ã‚µã‚¤ã‚ºãªã©ã®æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "# Model ã‚’ç”¨æ„\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# 1 ã¤ç›®ã® Prompt ã¨ Chain ã‚’ç”¨æ„\n",
        "cot_template = \"\"\"\n",
        "ä»¥ä¸‹ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "### è³ªå• ###\n",
        "{question}\n",
        "### è³ªå•çµ‚äº† ###\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§è€ƒãˆã¾ã—ã‚‡ã†ã€‚\n",
        "\"\"\"\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=cot_template,\n",
        ")\n",
        "cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n",
        "\n",
        "# 2 ã¤ç›®ã® Prompt ã¨ Chain ã‚’ç”¨æ„\n",
        "summarize_template = \"\"\"\n",
        "å…¥åŠ›ã‚’çµè«–ã ã‘ä¸€è¨€ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "### å…¥åŠ› ###\n",
        "{input}\n",
        "### å…¥åŠ›çµ‚äº† ###\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=summarize_template,\n",
        ")\n",
        "summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n",
        "\n",
        "# 2 ã¤ã® Chain ã‚’ç›´åˆ—ã«ç¹‹ã\n",
        "cot_summarize_chain = SimpleSequentialChain(\n",
        "    chains=[cot_chain, summarize_chain])\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "result = cot_summarize_chain(\n",
        "    \"ç§ã¯å¸‚å ´ã«è¡Œã£ã¦10å€‹ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸã€‚éš£äººã«2ã¤ã€ä¿®ç†å·¥ã«2ã¤æ¸¡ã—ã¾ã—ãŸã€‚ãã‚Œã‹ã‚‰5ã¤ã®ãƒªãƒ³ã‚´ã‚’è²·ã£ã¦1ã¤é£Ÿã¹ã¾ã—ãŸã€‚æ®‹ã‚Šã¯ä½•å€‹ã§ã™ã‹ï¼Ÿ\")\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "id": "BETIBsg104QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ebcca3-63bb-4dc6-9f00-a75483b6641a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "ä»¥ä¸‹ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
            "\n",
            "### è³ªå• ###\n",
            "ç§ã¯å¸‚å ´ã«è¡Œã£ã¦10å€‹ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸã€‚éš£äººã«2ã¤ã€ä¿®ç†å·¥ã«2ã¤æ¸¡ã—ã¾ã—ãŸã€‚ãã‚Œã‹ã‚‰5ã¤ã®ãƒªãƒ³ã‚´ã‚’è²·ã£ã¦1ã¤é£Ÿã¹ã¾ã—ãŸã€‚æ®‹ã‚Šã¯ä½•å€‹ã§ã™ã‹ï¼Ÿ\n",
            "### è³ªå•çµ‚äº† ###\n",
            "\n",
            "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§è€ƒãˆã¾ã—ã‚‡ã†ã€‚\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m1. æœ€åˆã«å¸‚å ´ã§10å€‹ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸã€‚\n",
            "2. éš£äººã«2ã¤ã®ãƒªãƒ³ã‚´ã‚’æ¸¡ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã¯8å€‹ã§ã™ã€‚\n",
            "3. ä¿®ç†å·¥ã«2ã¤ã®ãƒªãƒ³ã‚´ã‚’æ¸¡ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã¯6å€‹ã§ã™ã€‚\n",
            "4. ã•ã‚‰ã«5ã¤ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸã€‚æ®‹ã‚Šã¯11å€‹ã§ã™ã€‚\n",
            "5. 1ã¤ã®ãƒªãƒ³ã‚´ã‚’é£Ÿã¹ã¾ã—ãŸã€‚æ®‹ã‚Šã¯10å€‹ã§ã™ã€‚\n",
            "\n",
            "ã—ãŸãŒã£ã¦ã€æ®‹ã‚Šã®ãƒªãƒ³ã‚´ã¯10å€‹ã§ã™ã€‚\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "å…¥åŠ›ã‚’çµè«–ã ã‘ä¸€è¨€ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n",
            "\n",
            "### å…¥åŠ› ###\n",
            "1. æœ€åˆã«å¸‚å ´ã§10å€‹ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸã€‚\n",
            "2. éš£äººã«2ã¤ã®ãƒªãƒ³ã‚´ã‚’æ¸¡ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã¯8å€‹ã§ã™ã€‚\n",
            "3. ä¿®ç†å·¥ã«2ã¤ã®ãƒªãƒ³ã‚´ã‚’æ¸¡ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã¯6å€‹ã§ã™ã€‚\n",
            "4. ã•ã‚‰ã«5ã¤ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸã€‚æ®‹ã‚Šã¯11å€‹ã§ã™ã€‚\n",
            "5. 1ã¤ã®ãƒªãƒ³ã‚´ã‚’é£Ÿã¹ã¾ã—ãŸã€‚æ®‹ã‚Šã¯10å€‹ã§ã™ã€‚\n",
            "\n",
            "ã—ãŸãŒã£ã¦ã€æ®‹ã‚Šã®ãƒªãƒ³ã‚´ã¯10å€‹ã§ã™ã€‚\n",
            "### å…¥åŠ›çµ‚äº† ###\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mæœ€åˆã«å¸‚å ´ã§10å€‹ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸãŒã€æœ€çµ‚çš„ã«ã¯æ®‹ã‚Šã®ãƒªãƒ³ã‚´ã¯10å€‹ã§ã™ã€‚\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "æœ€åˆã«å¸‚å ´ã§10å€‹ã®ãƒªãƒ³ã‚´ã‚’è²·ã„ã¾ã—ãŸãŒã€æœ€çµ‚çš„ã«ã¯æ®‹ã‚Šã®ãƒªãƒ³ã‚´ã¯10å€‹ã§ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7. Prompts ã®ã‚ˆã‚Šé«˜åº¦ãªæ©Ÿèƒ½ï¼ˆOutput Parsesï¼‰"
      ],
      "metadata": {
        "id": "t1fNzjTRRl7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: List[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: List[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "template = \"\"\"æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "æ–™ç†å: {dish}\n",
        "\"\"\"\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"dish\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=prompt)\n",
        "\n",
        "output = chain.run(dish=\"ã‚«ãƒ¬ãƒ¼\")\n",
        "print(\"=== output ===\")\n",
        "print(output)\n",
        "\n",
        "recipe = parser.parse(output)\n",
        "print(\"=== recipe object ===\")\n",
        "print(recipe)\n"
      ],
      "metadata": {
        "id": "qkm0Ys9k4t3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65a1a21-5d43-4eef-f74b-4896c46cfb0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mæ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"description\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"steps to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "\n",
            "æ–™ç†å: ã‚«ãƒ¬ãƒ¼\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "=== output ===\n",
            "{\n",
            "  \"ingredients\": [\n",
            "    \"ç‰ã­ã\",\n",
            "    \"ã«ã‚“ã˜ã‚“\",\n",
            "    \"ã˜ã‚ƒãŒã„ã‚‚\",\n",
            "    \"è±šè‚‰\",\n",
            "    \"ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼\",\n",
            "    \"æ°´\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"ç‰ã­ãã€ã«ã‚“ã˜ã‚“ã€ã˜ã‚ƒãŒã„ã‚‚ã‚’åˆ‡ã‚‹\",\n",
            "    \"è±šè‚‰ã‚’ç‚’ã‚ã‚‹\",\n",
            "    \"é‡èœã‚’åŠ ãˆã¦ç‚’ã‚ã‚‹\",\n",
            "    \"æ°´ã‚’åŠ ãˆã¦ç…®è¾¼ã‚€\",\n",
            "    \"ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼ã‚’åŠ ãˆã¦æº¶ã‹ã™\",\n",
            "    \"ç…®è¾¼ã‚“ã§å®Œæˆ\"\n",
            "  ]\n",
            "}\n",
            "=== recipe object ===\n",
            "ingredients=['ç‰ã­ã', 'ã«ã‚“ã˜ã‚“', 'ã˜ã‚ƒãŒã„ã‚‚', 'è±šè‚‰', 'ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼', 'æ°´'] steps=['ç‰ã­ãã€ã«ã‚“ã˜ã‚“ã€ã˜ã‚ƒãŒã„ã‚‚ã‚’åˆ‡ã‚‹', 'è±šè‚‰ã‚’ç‚’ã‚ã‚‹', 'é‡èœã‚’åŠ ãˆã¦ç‚’ã‚ã‚‹', 'æ°´ã‚’åŠ ãˆã¦ç…®è¾¼ã‚€', 'ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼ã‚’åŠ ãˆã¦æº¶ã‹ã™', 'ç…®è¾¼ã‚“ã§å®Œæˆ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. LangChain å…¥é–€ï¼ˆå¾ŒåŠï¼‰"
      ],
      "metadata": {
        "id": "vkfHWlKnSjSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Indexes"
      ],
      "metadata": {
        "id": "prFUg4G3QfbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hwchase17/langchain.git"
      ],
      "metadata": {
        "id": "8VY_0R_VGcHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945654ad-247b-47a9-c30b-ba7016074ab4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'langchain'...\n",
            "remote: Enumerating objects: 111078, done.\u001b[K\n",
            "remote: Counting objects: 100% (44463/44463), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4388/4388), done.\u001b[K\n",
            "remote: Total 111078 (delta 41818), reused 40186 (delta 40075), pack-reused 66615\u001b[K\n",
            "Receiving objects: 100% (111078/111078), 152.67 MiB | 30.20 MiB/s, done.\n",
            "Resolving deltas: 100% (82295/82295), done.\n",
            "Updating files: 100% (5911/5911), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd langchain && git checkout v0.0.172"
      ],
      "metadata": {
        "id": "uhcQzqUqR1t3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942532bc-4c2f-40b9-a839-3a61f56ab3be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: switching to 'v0.0.172'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at a63ab7ded bump 172 (#4864)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet unstructured==0.6.6 tabulate==0.9.0 pdf2image==1.16.3 pytesseract==0.3.10 chromadb==0.3.23 tiktoken==0.4.0"
      ],
      "metadata": {
        "id": "znXiN1W1Rd8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1941c35-11be-436a-c8c5-9609f9a70d63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m964.5/964.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m730.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la langchain/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmqLWDTASA8E",
        "outputId": "866de8f4-d0c0-42e1-8645-d409779092e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 792\n",
            "drwxr-xr-x  8 root root   4096 Dec 22 02:02 .\n",
            "drwxr-xr-x  1 root root   4096 Dec 22 02:01 ..\n",
            "-rw-r--r--  1 root root    231 Dec 22 02:02 CITATION.cff\n",
            "drwxr-xr-x  2 root root   4096 Dec 22 02:02 .devcontainer\n",
            "-rw-r--r--  1 root root   1522 Dec 22 02:02 Dockerfile\n",
            "-rw-r--r--  1 root root     55 Dec 22 02:02 .dockerignore\n",
            "drwxr-xr-x 10 root root   4096 Dec 22 02:02 docs\n",
            "-rw-r--r--  1 root root    276 Dec 22 02:02 .flake8\n",
            "drwxr-xr-x  8 root root   4096 Dec 22 02:02 .git\n",
            "drwxr-xr-x  5 root root   4096 Dec 22 02:02 .github\n",
            "-rw-r--r--  1 root root   2052 Dec 22 02:02 .gitignore\n",
            "drwxr-xr-x 23 root root   4096 Dec 22 02:02 langchain\n",
            "-rw-r--r--  1 root root   1069 Dec 22 02:02 LICENSE\n",
            "-rw-r--r--  1 root root   2119 Dec 22 02:02 Makefile\n",
            "-rw-r--r--  1 root root 719304 Dec 22 02:02 poetry.lock\n",
            "-rw-r--r--  1 root root     73 Dec 22 02:01 poetry.toml\n",
            "-rw-r--r--  1 root root   8322 Dec 22 02:02 pyproject.toml\n",
            "-rw-r--r--  1 root root   5519 Dec 22 02:02 README.md\n",
            "-rw-r--r--  1 root root    612 Dec 22 02:02 .readthedocs.yaml\n",
            "drwxr-xr-x  5 root root   4096 Dec 22 02:02 tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la langchain/docs/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjmN-8tRSXkW",
        "outputId": "60c0904e-0279-4af0-ad04-81aba75c0c29"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 72\n",
            "drwxr-xr-x 10 root root 4096 Dec 22 02:02 .\n",
            "drwxr-xr-x  8 root root 4096 Dec 22 02:02 ..\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 additional_resources\n",
            "-rw-r--r--  1 root root 3626 Dec 22 02:02 conf.py\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 ecosystem\n",
            "-rw-r--r--  1 root root  865 Dec 22 02:02 ecosystem.rst\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 getting_started\n",
            "-rw-r--r--  1 root root 8054 Dec 22 02:02 index.rst\n",
            "-rw-r--r--  1 root root  795 Dec 22 02:02 make.bat\n",
            "-rw-r--r--  1 root root  673 Dec 22 02:02 Makefile\n",
            "drwxr-xr-x 10 root root 4096 Dec 22 02:02 modules\n",
            "drwxr-xr-x  3 root root 4096 Dec 22 02:02 reference\n",
            "-rw-r--r--  1 root root  450 Dec 22 02:02 reference.rst\n",
            "-rw-r--r--  1 root root  236 Dec 22 02:02 requirements.txt\n",
            "drwxr-xr-x  4 root root 4096 Dec 22 02:02 _static\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 tracing\n",
            "drwxr-xr-x 10 root root 4096 Dec 22 02:02 use_cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.md\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ],
      "metadata": {
        "id": "Q10Ff7vNQuV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2ec269-efb5-40cc-dbba-5a4869f195bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "result = index.query(\"LangChainã®æ¦‚è¦ã‚’1æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\", llm=chat)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "mJbppn-lRblc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04689dd-7211-4898-f894-7f6127be017c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\". For more concrete ideas on the latter, see this awesome paper.\n",
            "\n",
            "LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.\n",
            "\n",
            "â›“ï¸ IntroduÃ§Ã£o ao Langchain - #Cortes - Live DataHackers by Prof. JoÃ£o Gabriel Lima\n",
            "\n",
            "â›“ï¸ LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity\n",
            "\n",
            "â›“ï¸ KI schreibt krasses Youtube Skript ğŸ˜²ğŸ˜³ | LangChain Tutorial Deutsch by SimpleKI\n",
            "\n",
            "â›“ï¸ Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime\n",
            "\n",
            "â›“ï¸ QA over documents with Auto vector index selection with Langchain router chains by echohive\n",
            "\n",
            "â›“ï¸ Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox\n",
            "\n",
            "â›“ï¸ Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk\n",
            "\n",
            "â›“ï¸ LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra\n",
            "\n",
            "â›“ï¸ LANGCHAIN AI AUTONOMOUS AGENT WEB APP - ğŸ‘¶ BABY AGI ğŸ¤– with EMAIL AUTOMATION using DATABUTTON by Avra\n",
            "\n",
            "â›“ï¸ The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data\n",
            "\n",
            "Code Understanding\n",
            "\n",
            "Overview\n",
            "\n",
            "LangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\n",
            "\n",
            "Conversational Retriever Chain\n",
            "\n",
            "Conversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\n",
            "\n",
            "LangChain Workflow for Code Understanding and Generation\n",
            "\n",
            "Interacting with APIs\n",
            "\n",
            "Conceptual Guide\n",
            "\n",
            "Lots of data and information is stored behind APIs.\n",
            "This page covers all resources available in LangChain for working with APIs.\n",
            "\n",
            "Chains\n",
            "\n",
            "If you are just getting started, and you have relatively simple apis, you should get started with chains.\n",
            "Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you \n",
            "understand what is happening better.\n",
            "\n",
            "API Chain\n",
            "\n",
            "Agents\n",
            "\n",
            "Agents are more complex, and involve multiple queries to the LLM to understand what to do.\n",
            "The downside of agents are that you have less control. The upside is that they are more powerful,\n",
            "which allows you to use them on larger and more complex schemas.\n",
            "\n",
            "OpenAPI Agent\n",
            "Human: LangChainã®æ¦‚è¦ã‚’1æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "LangChainã¯GitHubã®ã‚³ãƒ¼ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’è§£æã—ã€ã‚³ãƒ¼ãƒ‰ã®ç†è§£ã€æ–‡è„ˆã«åŸºã¥ã„ãŸè³ªå•å¿œç­”ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. Memory"
      ],
      "metadata": {
        "id": "ARj8Y8_RdlD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_chat_completions(content):\n",
        "  url = \"https://api.openai.com/v1/chat/completions\"\n",
        "  headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "  }\n",
        "  data = {\n",
        "      \"model\": \"gpt-3.5-turbo\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"user\", \"content\": content}\n",
        "      ],\n",
        "      \"temperature\": 0,\n",
        "  }\n",
        "\n",
        "  response = requests.post(url=url, headers=headers, json=data)\n",
        "  print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "f7pxRwGnUDvn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_chat_completions(\"Hi! I'm Oshima!\")"
      ],
      "metadata": {
        "id": "V_OBzUQGVAwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd79b40-daab-44ba-c89f-99fb906c397c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YPLudA6ygZt57ggpwOf8bgh8J0zT\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703211298,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Hello Oshima! How can I assist you today?\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 14,\n",
            "    \"completion_tokens\": 11,\n",
            "    \"total_tokens\": 25\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_chat_completions(\"Do you know my name?\")"
      ],
      "metadata": {
        "id": "EYjoBKp8VAjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7425c2-fb15-422f-a7ed-27c314873fd9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YPMJKM2E8MnyRReBiwqf1l9tGcBP\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703211323,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"No, I do not know your name. As an AI language model, I don't have access to personal information unless you provide it to me in the course of our conversation.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 13,\n",
            "    \"completion_tokens\": 36,\n",
            "    \"total_tokens\": 49\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_chat_completions(\"\"\"Human: Hi! I'm Oshima!\n",
        "AI: Hello Oshima! I'm an AI language model. How can I assist you today?\n",
        "Human: Do you know my name?\n",
        "AI: \"\"\")"
      ],
      "metadata": {
        "id": "2ctwcsZ-VLdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88575e09-b366-4a0a-b215-147f1c4fe1d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YPNCxC7ubTqlpL3VtZaM334LUPvf\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703211378,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Yes, you mentioned that your name is Oshima.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 47,\n",
            "    \"completion_tokens\": 11,\n",
            "    \"total_tokens\": 58\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ],
      "metadata": {
        "id": "LjkhKO7nVeEK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "outputId": "84219752-9fbc-490c-86b4-8651b9434d72"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: My name is Masato\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: My name is Masato\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Hello Masato! It's nice to meet you. How can I assist you today?\n",
            "You: please tell me, S&P500 ticker\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Masato\n",
            "AI: Hello Masato! It's nice to meet you. How can I assist you today?\n",
            "Human: please tell me, S&P500 ticker\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: The ticker symbol for the S&P 500 is ^GSPC. It represents the performance of the 500 largest publicly traded companies in the United States. Is there anything else you would like to know?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-12cf370b334a>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mai_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI: {ai_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5. Agents"
      ],
      "metadata": {
        "id": "C0xTWbzzV4_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "tools = load_tools([\"terminal\"], llm=chat)\n",
        "agent_chain = initialize_agent(\n",
        "    tools, chat, agent=\"zero-shot-react-description\")\n",
        "\n",
        "result = agent_chain.run(\"What is your current directory?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "sbSTOTuKX7kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d21564f-5b59-40d6-82f8-c4627275a9f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "terminal: Run shell commands on this Linux machine.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [terminal]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: What is your current directory?\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI can use the \"pwd\" command to find out the current directory.\n",
            "Action: terminal\n",
            "Action Input: pwd\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m/content\n",
            "\u001b[0m\n",
            "Thought:\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "terminal: Run shell commands on this Linux machine.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [terminal]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: What is your current directory?\n",
            "Thought:I can use the \"pwd\" command to find out the current directory.\n",
            "Action: terminal\n",
            "Action Input: pwd\n",
            "Observation: /content\n",
            "\n",
            "Thought:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The current directory is /content.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The current directory is /content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoVCDfhhVaMb",
        "outputId": "f7f48abf-88b5-4764-e3d4-d6cd05a108e1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.8. Chat API ã«ç‰¹åŒ–ã—ãŸå®Ÿè£…"
      ],
      "metadata": {
        "id": "bqrKxwnhlstV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import openai\n",
        "\n",
        "langchain.verbose = True\n",
        "openai.log = \"debug\"\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ],
      "metadata": {
        "id": "jX1cLM9FJfbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "e9387460-b213-43aa-e03f-ac2083f492f1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Please tell me, Apple Inc. history\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Please tell me, Apple Inc. history\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n\\\\nHuman: Please tell me, Apple Inc. history\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2832 request_id=88db51129e7dabbebc1261380cac5546 response_code=200\n",
            "body='{\\n  \"id\": \"chatcmpl-8YPVj2w8EojA9nQ11fegBT15Ge67H\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1703211907,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Apple Inc. is a multinational technology company that was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne. The company\\'s headquarters are located in Cupertino, California. Apple is known for designing, developing, and selling consumer electronics, computer software, and online services.\\\\n\\\\nIn its early years, Apple gained popularity with the Apple II, one of the first successful personal computers. However, the company faced challenges in the 1980s, leading\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"length\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 72,\\n    \"completion_tokens\": 100,\\n    \"total_tokens\": 172\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Fri, 22 Dec 2023 02:25:10 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-nlomaspt3qpx4wffhp9qldqq\\', \\'openai-processing-ms\\': \\'2832\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'200\\', \\'x-ratelimit-limit-tokens\\': \\'40000\\', \\'x-ratelimit-limit-tokens_usage_based\\': \\'40000\\', \\'x-ratelimit-remaining-requests\\': \\'189\\', \\'x-ratelimit-remaining-tokens\\': \\'39824\\', \\'x-ratelimit-remaining-tokens_usage_based\\': \\'39824\\', \\'x-ratelimit-reset-requests\\': \\'1h17m30.252s\\', \\'x-ratelimit-reset-tokens\\': \\'264ms\\', \\'x-ratelimit-reset-tokens_usage_based\\': \\'264ms\\', \\'x-request-id\\': \\'88db51129e7dabbebc1261380cac5546\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8394e996080d0824-IAD\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Apple Inc. is a multinational technology company that was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne. The company's headquarters are located in Cupertino, California. Apple is known for designing, developing, and selling consumer electronics, computer software, and online services.\n",
            "\n",
            "In its early years, Apple gained popularity with the Apple II, one of the first successful personal computers. However, the company faced challenges in the 1980s, leading\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-635682e480ae>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mai_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI: {ai_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi! I'm Oshima!\")\n",
        "]\n",
        "\n",
        "result = chat(messages)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "X90oA5kIPGYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b75a1a3-6448-44d7-fcc9-1dfe526c8cf3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hi! I\\'m Oshima!\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hello Oshima! How can I assist you today?' additional_kwargs={} example=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=532 request_id=229b65ac2ec9666d77a0e13fcb11d3a8 response_code=200\n",
            "body='{\\n  \"id\": \"chatcmpl-8YPXehppeNfAbRGCHMcWlGB1OtQRt\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1703212026,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Hello Oshima! How can I assist you today?\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 24,\\n    \"completion_tokens\": 11,\\n    \"total_tokens\": 35\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Fri, 22 Dec 2023 02:27:06 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-nlomaspt3qpx4wffhp9qldqq\\', \\'openai-processing-ms\\': \\'532\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'200\\', \\'x-ratelimit-limit-tokens\\': \\'40000\\', \\'x-ratelimit-limit-tokens_usage_based\\': \\'40000\\', \\'x-ratelimit-remaining-requests\\': \\'187\\', \\'x-ratelimit-remaining-tokens\\': \\'39887\\', \\'x-ratelimit-remaining-tokens_usage_based\\': \\'39887\\', \\'x-ratelimit-reset-requests\\': \\'1h29m55.804s\\', \\'x-ratelimit-reset-tokens\\': \\'169ms\\', \\'x-ratelimit-reset-tokens_usage_based\\': \\'169ms\\', \\'x-request-id\\': \\'229b65ac2ec9666d77a0e13fcb11d3a8\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8394ec7a78810824-IAD\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    memory.chat_memory.add_user_message(user_message)\n",
        "\n",
        "    ai_message = chat(memory.chat_memory.messages)\n",
        "    memory.chat_memory.add_ai_message(ai_message.content)\n",
        "    print(f\"AI: {ai_message.content}\")\n"
      ],
      "metadata": {
        "id": "gZAE9dogmna_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "b79307e9-1c82-4040-f98b-a9124b34981b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Please tell me, Eron Musk history.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"Please tell me, Eron Musk history.\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=12361 request_id=01a7a4141f911a513d86ba658400c8af response_code=200\n",
            "body='{\\n  \"id\": \"chatcmpl-8YPYlllxoF7gwpBN17odHQmcO090B\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1703212095,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Elon Musk, not Eron Musk, is a well-known entrepreneur and business magnate. Here is a brief overview of his history:\\\\n\\\\nElon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa. He developed an early interest in computing and technology, teaching himself computer programming at a young age. Musk moved to Canada in the late 1980s to attend Queen\\'s University and later transferred to the University of Pennsylvania in the United States, where he earned dual bachelor\\'s degrees in physics and economics.\\\\n\\\\nAfter completing his studies, Musk co-founded Zip2, a software company that provided business directories and maps for newspapers. In 1999, Compaq acquired Zip2 for $307 million, providing Musk with his first significant financial success.\\\\n\\\\nMusk then went on to co-found X.com, an online payment company, in 1999. X.com eventually became PayPal, which revolutionized online payments. In 2002, eBay acquired PayPal for $1.5 billion, making Musk one of the largest shareholders.\\\\n\\\\nWith the financial success from PayPal, Musk turned his attention to various ventures. In 2002, he founded SpaceX (Space Exploration Technologies Corp.), with the goal of reducing space transportation costs and enabling the colonization of Mars. SpaceX has since achieved numerous milestones, including launching the first privately-funded spacecraft to reach orbit and successfully landing reusable rockets.\\\\n\\\\nIn 2004, Musk became the chairman and largest shareholder of Tesla Motors (now Tesla, Inc.), an electric vehicle and clean energy company. Under his leadership, Tesla has become a prominent player in the automotive industry, producing electric cars, energy storage solutions, and solar products.\\\\n\\\\nMusk has also been involved in other ventures, such as SolarCity (a solar energy services company), Neuralink (a neurotechnology company), and The Boring Company (focused on tunnel construction and infrastructure).\\\\n\\\\nThroughout his career, Musk has gained attention for his ambitious goals, innovative ideas, and outspoken personality. He has been recognized for his contributions to technology, space exploration, and sustainability.\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 16,\\n    \"completion_tokens\": 416,\\n    \"total_tokens\": 432\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Fri, 22 Dec 2023 02:28:27 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-nlomaspt3qpx4wffhp9qldqq\\', \\'openai-processing-ms\\': \\'12361\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'200\\', \\'x-ratelimit-limit-tokens\\': \\'40000\\', \\'x-ratelimit-limit-tokens_usage_based\\': \\'40000\\', \\'x-ratelimit-remaining-requests\\': \\'186\\', \\'x-ratelimit-remaining-tokens\\': \\'39974\\', \\'x-ratelimit-remaining-tokens_usage_based\\': \\'39974\\', \\'x-ratelimit-reset-requests\\': \\'1h35m58.974s\\', \\'x-ratelimit-reset-tokens\\': \\'39ms\\', \\'x-ratelimit-reset-tokens_usage_based\\': \\'39ms\\', \\'x-request-id\\': \\'01a7a4141f911a513d86ba658400c8af\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Set-Cookie\\': \\'__cf_bm=yCNPsq4N6nn2LeL2U.dWTbB3XTvVNvYrSnmJ0cBvEHo-1703212107-1-Ach28t25SolOJNhirE7Sq8bYdQ2YVx2VOdFSOHMKzKqlpRZqdGaW52dsw7cDxNgjISuaUoRQBCpq4Duy4eHUv9U=; path=/; expires=Fri, 22-Dec-23 02:58:27 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8394ee28892c0824-IAD\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Elon Musk, not Eron Musk, is a well-known entrepreneur and business magnate. Here is a brief overview of his history:\n",
            "\n",
            "Elon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa. He developed an early interest in computing and technology, teaching himself computer programming at a young age. Musk moved to Canada in the late 1980s to attend Queen's University and later transferred to the University of Pennsylvania in the United States, where he earned dual bachelor's degrees in physics and economics.\n",
            "\n",
            "After completing his studies, Musk co-founded Zip2, a software company that provided business directories and maps for newspapers. In 1999, Compaq acquired Zip2 for $307 million, providing Musk with his first significant financial success.\n",
            "\n",
            "Musk then went on to co-found X.com, an online payment company, in 1999. X.com eventually became PayPal, which revolutionized online payments. In 2002, eBay acquired PayPal for $1.5 billion, making Musk one of the largest shareholders.\n",
            "\n",
            "With the financial success from PayPal, Musk turned his attention to various ventures. In 2002, he founded SpaceX (Space Exploration Technologies Corp.), with the goal of reducing space transportation costs and enabling the colonization of Mars. SpaceX has since achieved numerous milestones, including launching the first privately-funded spacecraft to reach orbit and successfully landing reusable rockets.\n",
            "\n",
            "In 2004, Musk became the chairman and largest shareholder of Tesla Motors (now Tesla, Inc.), an electric vehicle and clean energy company. Under his leadership, Tesla has become a prominent player in the automotive industry, producing electric cars, energy storage solutions, and solar products.\n",
            "\n",
            "Musk has also been involved in other ventures, such as SolarCity (a solar energy services company), Neuralink (a neurotechnology company), and The Boring Company (focused on tunnel construction and infrastructure).\n",
            "\n",
            "Throughout his career, Musk has gained attention for his ambitious goals, innovative ideas, and outspoken personality. He has been recognized for his contributions to technology, space exploration, and sustainability.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f98a112a050c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyhA5HVJLGI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}