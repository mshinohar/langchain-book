{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshinohar/langchain-book/blob/main/udemy_langchain_ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Udemy講座「LangChainによるLLMアプリ開発入門」のコース前半のソースコード"
      ],
      "metadata": {
        "id": "0LSkqxoVN3J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GPT の API の基礎知識"
      ],
      "metadata": {
        "id": "IVs2pKa2ODmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. OpenAI の API キーを発行"
      ],
      "metadata": {
        "id": "mLGNNT3BOWFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "r0cuW-PeOD_s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. OpenAI の Completions API にふれる"
      ],
      "metadata": {
        "id": "bYVHgMnrOez3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://api.openai.com/v1/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "}\n",
        "data = {\n",
        "    # 動画内で使用している「text-davinci-003」は2024年1月4日に廃止予定です。\n",
        "    # そのため、こちらのコードで使用するモデルは、\n",
        "    # Completions APIで新しく利用可能になった「gpt-3.5-turbo-instruct」に変更しました。\n",
        "    \"model\": \"gpt-3.5-turbo-instruct\",\n",
        "    \"prompt\": \"hello!\",\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "response = requests.post(url=url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "2cPPmp7IOGbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60064ab-83e5-4a26-d371-955d51698bb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-8YP1PGObCiwKrXKjRDGzAiKnQTTSQ\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1703210027,\n",
            "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \"\\n\\nHello! How can I assist you?\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 2,\n",
            "    \"completion_tokens\": 9,\n",
            "    \"total_tokens\": 11\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. OpenAI の Chat API にふれる"
      ],
      "metadata": {
        "id": "WY7cd-EeOwiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://api.openai.com/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"hello!\"}\n",
        "    ],\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "response = requests.post(url=url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "bOzgI4KGOIcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785d766f-4efe-420a-c244-07e8c37eaeff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YP2EIifaAwWKfM3POpJFO8MhbI0f\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703210078,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Hello! How can I assist you today?\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 9,\n",
            "    \"completion_tokens\": 9,\n",
            "    \"total_tokens\": 18\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. LangChain 入門（前半）"
      ],
      "metadata": {
        "id": "UlDl6XqsOzFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Models（LLMs・Chat Models）"
      ],
      "metadata": {
        "id": "SVO4MzQxPJDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain==0.0.172 openai==0.27.6"
      ],
      "metadata": {
        "id": "lVxZFGSagE1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b12b0fd-832e-4b6d-be80-f8cc361d28b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.9/849.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# このセルで使用している「text-davinci-003」は2024年1月4日に廃止予定です。\n",
        "# その後、Completions APIでは「gpt-3.5-turbo-instruct」というモデルを使用することになります。\n",
        "#\n",
        "# ただし、このコースで使用しているLangChain v0.0.172は「gpt-3.5-turbo-instruct」に対応していないため、\n",
        "# 以下のコードのモデルを「gpt-3.5-turbo-instruct」に変更しても動作しません。\n",
        "# (新しいバージョンのLangChainであれば、「gpt-3.5-turbo-instruct」も使用可能です)\n",
        "#\n",
        "# LangChainのOpenAIクラスやCompletions APIはこのコースで以後使用しないため、\n",
        "# このコードが動作しなくてもあまり気にせず、講座の続きに進んでください。\n",
        "# (もしもOpenAIクラスを「gpt-3.5-turbo-instruct」で動かしてみたい場合は、\n",
        "# Google Colabを別途用意して、LangChainの新しいバージョンをインストールしてください)\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "result = llm.predict(\"自己紹介してください。\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "gJHDUhwwgKc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df7a56-d79b-43e9-9157-ec0956ba5911"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "はじめまして、私は○○と申します。現在、○○大学で学んでいます。趣味は料理や旅行などです。特に料理は好きで、毎日新しいレシピを試しています。今までに海外旅行も何回かしていて、異文化に触れるのが大好きです。今後も新しいことに挑戦していきたいと思っています。よろしくお願いします。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "result = chat.predict(\"自己紹介してください。\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "OShyvz70hRD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06038815-899a-4c51-eb5d-291cf718ef74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは、私はAIです。私はOpenAIが開発した自然言語処理モデルです。私の目的は、ユーザーが質問や要求をすると、最善の回答や応答を提供することです。私は様々なトピックについての情報を持っており、文法やスペルの修正も行うことができます。どのようにお手伝いできますか？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Prompts（Prompt Templates）"
      ],
      "metadata": {
        "id": "z9XTxsN8xitv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "次のコマンドの概要を説明してください。\n",
        "\n",
        "コマンド: {command}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"command\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "result = prompt.format(command=\"ls\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "-sZIx1e8iwJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ef6fb1-ec59-46da-f830-9a0a5d1a18cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "次のコマンドの概要を説明してください。\n",
            "\n",
            "コマンド: ls\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Chains"
      ],
      "metadata": {
        "id": "u0eKtJGu0vX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "# Model を用意\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Prompt を用意\n",
        "template = \"\"\"\n",
        "次のコマンドの概要を説明してください。\n",
        "\n",
        "コマンド: {command}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"command\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Chain を作成\n",
        "chain = LLMChain(llm=chat, prompt=prompt)\n",
        "\n",
        "# 実行\n",
        "result = chain.run(\"ls\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "9QyYR_Iixyuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059189c6-10b6-4d35-edc6-1878ebc85653"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "次のコマンドの概要を説明してください。\n",
            "\n",
            "コマンド: ls\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "lsコマンドは、UnixやUnix系のオペレーティングシステムで使用されるコマンドで、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するために使用されます。デフォルトでは、現在のディレクトリの内容が表示されますが、オプションを使用して他のディレクトリの内容を表示することもできます。ファイルやディレクトリの名前、パーミッション、所有者、サイズなどの情報が表示されます。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "# Model を用意\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# 1 つ目の Prompt と Chain を用意\n",
        "cot_template = \"\"\"\n",
        "以下の質問に回答してください。\n",
        "\n",
        "### 質問 ###\n",
        "{question}\n",
        "### 質問終了 ###\n",
        "\n",
        "ステップバイステップで考えましょう。\n",
        "\"\"\"\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=cot_template,\n",
        ")\n",
        "cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n",
        "\n",
        "# 2 つ目の Prompt と Chain を用意\n",
        "summarize_template = \"\"\"\n",
        "入力を結論だけ一言に要約してください。\n",
        "\n",
        "### 入力 ###\n",
        "{input}\n",
        "### 入力終了 ###\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=summarize_template,\n",
        ")\n",
        "summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n",
        "\n",
        "# 2 つの Chain を直列に繋ぐ\n",
        "cot_summarize_chain = SimpleSequentialChain(\n",
        "    chains=[cot_chain, summarize_chain])\n",
        "\n",
        "# 実行\n",
        "result = cot_summarize_chain(\n",
        "    \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\")\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "id": "BETIBsg104QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ebcca3-63bb-4dc6-9f00-a75483b6641a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "以下の質問に回答してください。\n",
            "\n",
            "### 質問 ###\n",
            "私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\n",
            "### 質問終了 ###\n",
            "\n",
            "ステップバイステップで考えましょう。\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m1. 最初に市場で10個のリンゴを買いました。\n",
            "2. 隣人に2つのリンゴを渡しました。残りは8個です。\n",
            "3. 修理工に2つのリンゴを渡しました。残りは6個です。\n",
            "4. さらに5つのリンゴを買いました。残りは11個です。\n",
            "5. 1つのリンゴを食べました。残りは10個です。\n",
            "\n",
            "したがって、残りのリンゴは10個です。\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "入力を結論だけ一言に要約してください。\n",
            "\n",
            "### 入力 ###\n",
            "1. 最初に市場で10個のリンゴを買いました。\n",
            "2. 隣人に2つのリンゴを渡しました。残りは8個です。\n",
            "3. 修理工に2つのリンゴを渡しました。残りは6個です。\n",
            "4. さらに5つのリンゴを買いました。残りは11個です。\n",
            "5. 1つのリンゴを食べました。残りは10個です。\n",
            "\n",
            "したがって、残りのリンゴは10個です。\n",
            "### 入力終了 ###\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m最初に市場で10個のリンゴを買いましたが、最終的には残りのリンゴは10個です。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "最初に市場で10個のリンゴを買いましたが、最終的には残りのリンゴは10個です。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7. Prompts のより高度な機能（Output Parses）"
      ],
      "metadata": {
        "id": "t1fNzjTRRl7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: List[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: List[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "template = \"\"\"料理のレシピを教えてください。\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"dish\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=prompt)\n",
        "\n",
        "output = chain.run(dish=\"カレー\")\n",
        "print(\"=== output ===\")\n",
        "print(output)\n",
        "\n",
        "recipe = parser.parse(output)\n",
        "print(\"=== recipe object ===\")\n",
        "print(recipe)\n"
      ],
      "metadata": {
        "id": "qkm0Ys9k4t3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65a1a21-5d43-4eef-f74b-4896c46cfb0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m料理のレシピを教えてください。\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"description\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"steps to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "\n",
            "料理名: カレー\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "=== output ===\n",
            "{\n",
            "  \"ingredients\": [\n",
            "    \"玉ねぎ\",\n",
            "    \"にんじん\",\n",
            "    \"じゃがいも\",\n",
            "    \"豚肉\",\n",
            "    \"カレールー\",\n",
            "    \"水\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"玉ねぎ、にんじん、じゃがいもを切る\",\n",
            "    \"豚肉を炒める\",\n",
            "    \"野菜を加えて炒める\",\n",
            "    \"水を加えて煮込む\",\n",
            "    \"カレールーを加えて溶かす\",\n",
            "    \"煮込んで完成\"\n",
            "  ]\n",
            "}\n",
            "=== recipe object ===\n",
            "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '豚肉', 'カレールー', '水'] steps=['玉ねぎ、にんじん、じゃがいもを切る', '豚肉を炒める', '野菜を加えて炒める', '水を加えて煮込む', 'カレールーを加えて溶かす', '煮込んで完成']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. LangChain 入門（後半）"
      ],
      "metadata": {
        "id": "vkfHWlKnSjSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Indexes"
      ],
      "metadata": {
        "id": "prFUg4G3QfbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hwchase17/langchain.git"
      ],
      "metadata": {
        "id": "8VY_0R_VGcHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945654ad-247b-47a9-c30b-ba7016074ab4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'langchain'...\n",
            "remote: Enumerating objects: 111078, done.\u001b[K\n",
            "remote: Counting objects: 100% (44463/44463), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4388/4388), done.\u001b[K\n",
            "remote: Total 111078 (delta 41818), reused 40186 (delta 40075), pack-reused 66615\u001b[K\n",
            "Receiving objects: 100% (111078/111078), 152.67 MiB | 30.20 MiB/s, done.\n",
            "Resolving deltas: 100% (82295/82295), done.\n",
            "Updating files: 100% (5911/5911), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd langchain && git checkout v0.0.172"
      ],
      "metadata": {
        "id": "uhcQzqUqR1t3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942532bc-4c2f-40b9-a839-3a61f56ab3be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: switching to 'v0.0.172'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at a63ab7ded bump 172 (#4864)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet unstructured==0.6.6 tabulate==0.9.0 pdf2image==1.16.3 pytesseract==0.3.10 chromadb==0.3.23 tiktoken==0.4.0"
      ],
      "metadata": {
        "id": "znXiN1W1Rd8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1941c35-11be-436a-c8c5-9609f9a70d63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.5/964.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m730.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la langchain/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmqLWDTASA8E",
        "outputId": "866de8f4-d0c0-42e1-8645-d409779092e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 792\n",
            "drwxr-xr-x  8 root root   4096 Dec 22 02:02 .\n",
            "drwxr-xr-x  1 root root   4096 Dec 22 02:01 ..\n",
            "-rw-r--r--  1 root root    231 Dec 22 02:02 CITATION.cff\n",
            "drwxr-xr-x  2 root root   4096 Dec 22 02:02 .devcontainer\n",
            "-rw-r--r--  1 root root   1522 Dec 22 02:02 Dockerfile\n",
            "-rw-r--r--  1 root root     55 Dec 22 02:02 .dockerignore\n",
            "drwxr-xr-x 10 root root   4096 Dec 22 02:02 docs\n",
            "-rw-r--r--  1 root root    276 Dec 22 02:02 .flake8\n",
            "drwxr-xr-x  8 root root   4096 Dec 22 02:02 .git\n",
            "drwxr-xr-x  5 root root   4096 Dec 22 02:02 .github\n",
            "-rw-r--r--  1 root root   2052 Dec 22 02:02 .gitignore\n",
            "drwxr-xr-x 23 root root   4096 Dec 22 02:02 langchain\n",
            "-rw-r--r--  1 root root   1069 Dec 22 02:02 LICENSE\n",
            "-rw-r--r--  1 root root   2119 Dec 22 02:02 Makefile\n",
            "-rw-r--r--  1 root root 719304 Dec 22 02:02 poetry.lock\n",
            "-rw-r--r--  1 root root     73 Dec 22 02:01 poetry.toml\n",
            "-rw-r--r--  1 root root   8322 Dec 22 02:02 pyproject.toml\n",
            "-rw-r--r--  1 root root   5519 Dec 22 02:02 README.md\n",
            "-rw-r--r--  1 root root    612 Dec 22 02:02 .readthedocs.yaml\n",
            "drwxr-xr-x  5 root root   4096 Dec 22 02:02 tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la langchain/docs/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjmN-8tRSXkW",
        "outputId": "60c0904e-0279-4af0-ad04-81aba75c0c29"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 72\n",
            "drwxr-xr-x 10 root root 4096 Dec 22 02:02 .\n",
            "drwxr-xr-x  8 root root 4096 Dec 22 02:02 ..\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 additional_resources\n",
            "-rw-r--r--  1 root root 3626 Dec 22 02:02 conf.py\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 ecosystem\n",
            "-rw-r--r--  1 root root  865 Dec 22 02:02 ecosystem.rst\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 getting_started\n",
            "-rw-r--r--  1 root root 8054 Dec 22 02:02 index.rst\n",
            "-rw-r--r--  1 root root  795 Dec 22 02:02 make.bat\n",
            "-rw-r--r--  1 root root  673 Dec 22 02:02 Makefile\n",
            "drwxr-xr-x 10 root root 4096 Dec 22 02:02 modules\n",
            "drwxr-xr-x  3 root root 4096 Dec 22 02:02 reference\n",
            "-rw-r--r--  1 root root  450 Dec 22 02:02 reference.rst\n",
            "-rw-r--r--  1 root root  236 Dec 22 02:02 requirements.txt\n",
            "drwxr-xr-x  4 root root 4096 Dec 22 02:02 _static\n",
            "drwxr-xr-x  2 root root 4096 Dec 22 02:02 tracing\n",
            "drwxr-xr-x 10 root root 4096 Dec 22 02:02 use_cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.md\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ],
      "metadata": {
        "id": "Q10Ff7vNQuV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2ec269-efb5-40cc-dbba-5a4869f195bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "result = index.query(\"LangChainの概要を1文で説明してください。\", llm=chat)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "mJbppn-lRblc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04689dd-7211-4898-f894-7f6127be017c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\". For more concrete ideas on the latter, see this awesome paper.\n",
            "\n",
            "LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.\n",
            "\n",
            "⛓️ Introdução ao Langchain - #Cortes - Live DataHackers by Prof. João Gabriel Lima\n",
            "\n",
            "⛓️ LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity\n",
            "\n",
            "⛓️ KI schreibt krasses Youtube Skript 😲😳 | LangChain Tutorial Deutsch by SimpleKI\n",
            "\n",
            "⛓️ Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime\n",
            "\n",
            "⛓️ QA over documents with Auto vector index selection with Langchain router chains by echohive\n",
            "\n",
            "⛓️ Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox\n",
            "\n",
            "⛓️ Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk\n",
            "\n",
            "⛓️ LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra\n",
            "\n",
            "⛓️ LANGCHAIN AI AUTONOMOUS AGENT WEB APP - 👶 BABY AGI 🤖 with EMAIL AUTOMATION using DATABUTTON by Avra\n",
            "\n",
            "⛓️ The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data\n",
            "\n",
            "Code Understanding\n",
            "\n",
            "Overview\n",
            "\n",
            "LangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\n",
            "\n",
            "Conversational Retriever Chain\n",
            "\n",
            "Conversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\n",
            "\n",
            "LangChain Workflow for Code Understanding and Generation\n",
            "\n",
            "Interacting with APIs\n",
            "\n",
            "Conceptual Guide\n",
            "\n",
            "Lots of data and information is stored behind APIs.\n",
            "This page covers all resources available in LangChain for working with APIs.\n",
            "\n",
            "Chains\n",
            "\n",
            "If you are just getting started, and you have relatively simple apis, you should get started with chains.\n",
            "Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you \n",
            "understand what is happening better.\n",
            "\n",
            "API Chain\n",
            "\n",
            "Agents\n",
            "\n",
            "Agents are more complex, and involve multiple queries to the LLM to understand what to do.\n",
            "The downside of agents are that you have less control. The upside is that they are more powerful,\n",
            "which allows you to use them on larger and more complex schemas.\n",
            "\n",
            "OpenAPI Agent\n",
            "Human: LangChainの概要を1文で説明してください。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "LangChainはGitHubのコードリポジトリを解析し、コードの理解、文脈に基づいた質問応答、コード生成をサポートするツールです。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. Memory"
      ],
      "metadata": {
        "id": "ARj8Y8_RdlD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_chat_completions(content):\n",
        "  url = \"https://api.openai.com/v1/chat/completions\"\n",
        "  headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "  }\n",
        "  data = {\n",
        "      \"model\": \"gpt-3.5-turbo\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"user\", \"content\": content}\n",
        "      ],\n",
        "      \"temperature\": 0,\n",
        "  }\n",
        "\n",
        "  response = requests.post(url=url, headers=headers, json=data)\n",
        "  print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "f7pxRwGnUDvn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_chat_completions(\"Hi! I'm Oshima!\")"
      ],
      "metadata": {
        "id": "V_OBzUQGVAwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd79b40-daab-44ba-c89f-99fb906c397c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YPLudA6ygZt57ggpwOf8bgh8J0zT\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703211298,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Hello Oshima! How can I assist you today?\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 14,\n",
            "    \"completion_tokens\": 11,\n",
            "    \"total_tokens\": 25\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_chat_completions(\"Do you know my name?\")"
      ],
      "metadata": {
        "id": "EYjoBKp8VAjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7425c2-fb15-422f-a7ed-27c314873fd9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YPMJKM2E8MnyRReBiwqf1l9tGcBP\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703211323,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"No, I do not know your name. As an AI language model, I don't have access to personal information unless you provide it to me in the course of our conversation.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 13,\n",
            "    \"completion_tokens\": 36,\n",
            "    \"total_tokens\": 49\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_chat_completions(\"\"\"Human: Hi! I'm Oshima!\n",
        "AI: Hello Oshima! I'm an AI language model. How can I assist you today?\n",
        "Human: Do you know my name?\n",
        "AI: \"\"\")"
      ],
      "metadata": {
        "id": "2ctwcsZ-VLdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88575e09-b366-4a0a-b215-147f1c4fe1d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YPNCxC7ubTqlpL3VtZaM334LUPvf\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1703211378,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Yes, you mentioned that your name is Oshima.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 47,\n",
            "    \"completion_tokens\": 11,\n",
            "    \"total_tokens\": 58\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ],
      "metadata": {
        "id": "LjkhKO7nVeEK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "outputId": "84219752-9fbc-490c-86b4-8651b9434d72"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: My name is Masato\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: My name is Masato\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Hello Masato! It's nice to meet you. How can I assist you today?\n",
            "You: please tell me, S&P500 ticker\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Masato\n",
            "AI: Hello Masato! It's nice to meet you. How can I assist you today?\n",
            "Human: please tell me, S&P500 ticker\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: The ticker symbol for the S&P 500 is ^GSPC. It represents the performance of the 500 largest publicly traded companies in the United States. Is there anything else you would like to know?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-12cf370b334a>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mai_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI: {ai_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5. Agents"
      ],
      "metadata": {
        "id": "C0xTWbzzV4_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "tools = load_tools([\"terminal\"], llm=chat)\n",
        "agent_chain = initialize_agent(\n",
        "    tools, chat, agent=\"zero-shot-react-description\")\n",
        "\n",
        "result = agent_chain.run(\"What is your current directory?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "sbSTOTuKX7kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d21564f-5b59-40d6-82f8-c4627275a9f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "terminal: Run shell commands on this Linux machine.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [terminal]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: What is your current directory?\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI can use the \"pwd\" command to find out the current directory.\n",
            "Action: terminal\n",
            "Action Input: pwd\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m/content\n",
            "\u001b[0m\n",
            "Thought:\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "terminal: Run shell commands on this Linux machine.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [terminal]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: What is your current directory?\n",
            "Thought:I can use the \"pwd\" command to find out the current directory.\n",
            "Action: terminal\n",
            "Action Input: pwd\n",
            "Observation: /content\n",
            "\n",
            "Thought:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The current directory is /content.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The current directory is /content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoVCDfhhVaMb",
        "outputId": "f7f48abf-88b5-4764-e3d4-d6cd05a108e1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.8. Chat API に特化した実装"
      ],
      "metadata": {
        "id": "bqrKxwnhlstV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import openai\n",
        "\n",
        "langchain.verbose = True\n",
        "openai.log = \"debug\"\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ],
      "metadata": {
        "id": "jX1cLM9FJfbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "e9387460-b213-43aa-e03f-ac2083f492f1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Please tell me, Apple Inc. history\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Please tell me, Apple Inc. history\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n\\\\nHuman: Please tell me, Apple Inc. history\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2832 request_id=88db51129e7dabbebc1261380cac5546 response_code=200\n",
            "body='{\\n  \"id\": \"chatcmpl-8YPVj2w8EojA9nQ11fegBT15Ge67H\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1703211907,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Apple Inc. is a multinational technology company that was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne. The company\\'s headquarters are located in Cupertino, California. Apple is known for designing, developing, and selling consumer electronics, computer software, and online services.\\\\n\\\\nIn its early years, Apple gained popularity with the Apple II, one of the first successful personal computers. However, the company faced challenges in the 1980s, leading\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"length\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 72,\\n    \"completion_tokens\": 100,\\n    \"total_tokens\": 172\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Fri, 22 Dec 2023 02:25:10 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-nlomaspt3qpx4wffhp9qldqq\\', \\'openai-processing-ms\\': \\'2832\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'200\\', \\'x-ratelimit-limit-tokens\\': \\'40000\\', \\'x-ratelimit-limit-tokens_usage_based\\': \\'40000\\', \\'x-ratelimit-remaining-requests\\': \\'189\\', \\'x-ratelimit-remaining-tokens\\': \\'39824\\', \\'x-ratelimit-remaining-tokens_usage_based\\': \\'39824\\', \\'x-ratelimit-reset-requests\\': \\'1h17m30.252s\\', \\'x-ratelimit-reset-tokens\\': \\'264ms\\', \\'x-ratelimit-reset-tokens_usage_based\\': \\'264ms\\', \\'x-request-id\\': \\'88db51129e7dabbebc1261380cac5546\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8394e996080d0824-IAD\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Apple Inc. is a multinational technology company that was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne. The company's headquarters are located in Cupertino, California. Apple is known for designing, developing, and selling consumer electronics, computer software, and online services.\n",
            "\n",
            "In its early years, Apple gained popularity with the Apple II, one of the first successful personal computers. However, the company faced challenges in the 1980s, leading\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-635682e480ae>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mai_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI: {ai_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi! I'm Oshima!\")\n",
        "]\n",
        "\n",
        "result = chat(messages)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "X90oA5kIPGYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b75a1a3-6448-44d7-fcc9-1dfe526c8cf3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hi! I\\'m Oshima!\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hello Oshima! How can I assist you today?' additional_kwargs={} example=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=532 request_id=229b65ac2ec9666d77a0e13fcb11d3a8 response_code=200\n",
            "body='{\\n  \"id\": \"chatcmpl-8YPXehppeNfAbRGCHMcWlGB1OtQRt\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1703212026,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Hello Oshima! How can I assist you today?\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 24,\\n    \"completion_tokens\": 11,\\n    \"total_tokens\": 35\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Fri, 22 Dec 2023 02:27:06 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-nlomaspt3qpx4wffhp9qldqq\\', \\'openai-processing-ms\\': \\'532\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'200\\', \\'x-ratelimit-limit-tokens\\': \\'40000\\', \\'x-ratelimit-limit-tokens_usage_based\\': \\'40000\\', \\'x-ratelimit-remaining-requests\\': \\'187\\', \\'x-ratelimit-remaining-tokens\\': \\'39887\\', \\'x-ratelimit-remaining-tokens_usage_based\\': \\'39887\\', \\'x-ratelimit-reset-requests\\': \\'1h29m55.804s\\', \\'x-ratelimit-reset-tokens\\': \\'169ms\\', \\'x-ratelimit-reset-tokens_usage_based\\': \\'169ms\\', \\'x-request-id\\': \\'229b65ac2ec9666d77a0e13fcb11d3a8\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8394ec7a78810824-IAD\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    memory.chat_memory.add_user_message(user_message)\n",
        "\n",
        "    ai_message = chat(memory.chat_memory.messages)\n",
        "    memory.chat_memory.add_ai_message(ai_message.content)\n",
        "    print(f\"AI: {ai_message.content}\")\n"
      ],
      "metadata": {
        "id": "gZAE9dogmna_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "b79307e9-1c82-4040-f98b-a9124b34981b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Please tell me, Eron Musk history.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"Please tell me, Eron Musk history.\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=12361 request_id=01a7a4141f911a513d86ba658400c8af response_code=200\n",
            "body='{\\n  \"id\": \"chatcmpl-8YPYlllxoF7gwpBN17odHQmcO090B\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1703212095,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Elon Musk, not Eron Musk, is a well-known entrepreneur and business magnate. Here is a brief overview of his history:\\\\n\\\\nElon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa. He developed an early interest in computing and technology, teaching himself computer programming at a young age. Musk moved to Canada in the late 1980s to attend Queen\\'s University and later transferred to the University of Pennsylvania in the United States, where he earned dual bachelor\\'s degrees in physics and economics.\\\\n\\\\nAfter completing his studies, Musk co-founded Zip2, a software company that provided business directories and maps for newspapers. In 1999, Compaq acquired Zip2 for $307 million, providing Musk with his first significant financial success.\\\\n\\\\nMusk then went on to co-found X.com, an online payment company, in 1999. X.com eventually became PayPal, which revolutionized online payments. In 2002, eBay acquired PayPal for $1.5 billion, making Musk one of the largest shareholders.\\\\n\\\\nWith the financial success from PayPal, Musk turned his attention to various ventures. In 2002, he founded SpaceX (Space Exploration Technologies Corp.), with the goal of reducing space transportation costs and enabling the colonization of Mars. SpaceX has since achieved numerous milestones, including launching the first privately-funded spacecraft to reach orbit and successfully landing reusable rockets.\\\\n\\\\nIn 2004, Musk became the chairman and largest shareholder of Tesla Motors (now Tesla, Inc.), an electric vehicle and clean energy company. Under his leadership, Tesla has become a prominent player in the automotive industry, producing electric cars, energy storage solutions, and solar products.\\\\n\\\\nMusk has also been involved in other ventures, such as SolarCity (a solar energy services company), Neuralink (a neurotechnology company), and The Boring Company (focused on tunnel construction and infrastructure).\\\\n\\\\nThroughout his career, Musk has gained attention for his ambitious goals, innovative ideas, and outspoken personality. He has been recognized for his contributions to technology, space exploration, and sustainability.\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 16,\\n    \"completion_tokens\": 416,\\n    \"total_tokens\": 432\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Fri, 22 Dec 2023 02:28:27 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-nlomaspt3qpx4wffhp9qldqq\\', \\'openai-processing-ms\\': \\'12361\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'200\\', \\'x-ratelimit-limit-tokens\\': \\'40000\\', \\'x-ratelimit-limit-tokens_usage_based\\': \\'40000\\', \\'x-ratelimit-remaining-requests\\': \\'186\\', \\'x-ratelimit-remaining-tokens\\': \\'39974\\', \\'x-ratelimit-remaining-tokens_usage_based\\': \\'39974\\', \\'x-ratelimit-reset-requests\\': \\'1h35m58.974s\\', \\'x-ratelimit-reset-tokens\\': \\'39ms\\', \\'x-ratelimit-reset-tokens_usage_based\\': \\'39ms\\', \\'x-request-id\\': \\'01a7a4141f911a513d86ba658400c8af\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Set-Cookie\\': \\'__cf_bm=yCNPsq4N6nn2LeL2U.dWTbB3XTvVNvYrSnmJ0cBvEHo-1703212107-1-Ach28t25SolOJNhirE7Sq8bYdQ2YVx2VOdFSOHMKzKqlpRZqdGaW52dsw7cDxNgjISuaUoRQBCpq4Duy4eHUv9U=; path=/; expires=Fri, 22-Dec-23 02:58:27 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8394ee28892c0824-IAD\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Elon Musk, not Eron Musk, is a well-known entrepreneur and business magnate. Here is a brief overview of his history:\n",
            "\n",
            "Elon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa. He developed an early interest in computing and technology, teaching himself computer programming at a young age. Musk moved to Canada in the late 1980s to attend Queen's University and later transferred to the University of Pennsylvania in the United States, where he earned dual bachelor's degrees in physics and economics.\n",
            "\n",
            "After completing his studies, Musk co-founded Zip2, a software company that provided business directories and maps for newspapers. In 1999, Compaq acquired Zip2 for $307 million, providing Musk with his first significant financial success.\n",
            "\n",
            "Musk then went on to co-found X.com, an online payment company, in 1999. X.com eventually became PayPal, which revolutionized online payments. In 2002, eBay acquired PayPal for $1.5 billion, making Musk one of the largest shareholders.\n",
            "\n",
            "With the financial success from PayPal, Musk turned his attention to various ventures. In 2002, he founded SpaceX (Space Exploration Technologies Corp.), with the goal of reducing space transportation costs and enabling the colonization of Mars. SpaceX has since achieved numerous milestones, including launching the first privately-funded spacecraft to reach orbit and successfully landing reusable rockets.\n",
            "\n",
            "In 2004, Musk became the chairman and largest shareholder of Tesla Motors (now Tesla, Inc.), an electric vehicle and clean energy company. Under his leadership, Tesla has become a prominent player in the automotive industry, producing electric cars, energy storage solutions, and solar products.\n",
            "\n",
            "Musk has also been involved in other ventures, such as SolarCity (a solar energy services company), Neuralink (a neurotechnology company), and The Boring Company (focused on tunnel construction and infrastructure).\n",
            "\n",
            "Throughout his career, Musk has gained attention for his ambitious goals, innovative ideas, and outspoken personality. He has been recognized for his contributions to technology, space exploration, and sustainability.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f98a112a050c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyhA5HVJLGI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}